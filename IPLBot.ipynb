{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40981,
     "status": "ok",
     "timestamp": 1654162224659,
     "user": {
      "displayName": "PRIYANKA MAHOR",
      "userId": "00901469365587543675"
     },
     "user_tz": -330
    },
    "id": "MSf5GWJjRBD0",
    "outputId": "da75645d-86a6-4cfd-bbb0-d4597169cc58"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#1 Mounting Google drive with Google Colab \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m json_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/My Drive/iplbot\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#1 Mounting Google drive with Google Colab \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "json_data='iplbot' #IPLChatBotSystem is a folder which have json file used for training Our Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 769,
     "status": "ok",
     "timestamp": 1654162235727,
     "user": {
      "displayName": "PRIYANKA MAHOR",
      "userId": "00901469365587543675"
     },
     "user_tz": -330
    },
    "id": "LVctJsfKRUQ-",
    "outputId": "c535c31b-8bd0-48b0-ba40-edce70aac074"
   },
   "outputs": [],
   "source": [
    "#2 Importing Relevant Libraries\n",
    "\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1808,
     "status": "ok",
     "timestamp": 1654162984947,
     "user": {
      "displayName": "PRIYANKA MAHOR",
      "userId": "00901469365587543675"
     },
     "user_tz": -330
    },
    "id": "VTzd05PURYkK",
    "outputId": "1dd93581-72ec-4857-c80f-9b927ce69f65"
   },
   "outputs": [],
   "source": [
    "#3 Loading the Dataset: intents.json\n",
    "\n",
    "data_json = open('iplbot/data_word.json').read()     #Json file is opened and reads content from it\n",
    "raw_data = json.loads(data_json)                                                            #Json data is loaded using loads method ,this method convert json data to python dict\n",
    "type(raw_data)                                                                              #Return type \n",
    "raw_data                                                                                   #Printing Data at console \n",
    "#print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2977,
     "status": "ok",
     "timestamp": 1654162992939,
     "user": {
      "displayName": "PRIYANKA MAHOR",
      "userId": "00901469365587543675"
     },
     "user_tz": -330
    },
    "id": "UIh1zHxfRdOn"
   },
   "outputs": [],
   "source": [
    "#4 Extracting data_X(features) and data_Y(Target)\n",
    "\n",
    "words = [] #For Bow model/ vocabulary for patterns\n",
    "classes = [] #For Bow  model/ vocabulary for tags\n",
    "data_X = [] #For storing each pattern\n",
    "data_y = [] #For storing tag corresponding to each pattern in data_X \n",
    "# Iterating over all the intents\n",
    "\n",
    "for intent in raw_data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern) # tokenize each pattern \n",
    "        words.extend(tokens) #and append tokens to words\n",
    "        data_X.append(pattern) #appending pattern to data_X\n",
    "        data_y.append(intent[\"tag\"]) ,# appending the associated tag to each pattern \n",
    "    \n",
    "    # adding the tag to the classes if it's not there already \n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "# initializing lemmatizer to get stem of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize all the words in the vocab and convert them to lowercase\n",
    "# if the words don't appear in punctuation\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1654163006665,
     "user": {
      "displayName": "PRIYANKA MAHOR",
      "userId": "00901469365587543675"
     },
     "user_tz": -330
    },
    "id": "xfNNUVz0RiFR"
   },
   "outputs": [],
   "source": [
    "# 5 Text to Numbers\n",
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "# creating the bag of words model\n",
    "for idx, doc in enumerate(data_X):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "    # mark the index of class that the current pattern is associated\n",
    "    # to\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(data_y[idx])] = 1\n",
    "    # add the one hot encoded BoW and associated classes to training \n",
    "    training.append([bow, output_row])\n",
    "# shuffle the data and convert it to an array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "# split the features and target labels\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_Y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9079,
     "status": "ok",
     "timestamp": 1654163021249,
     "user": {
      "displayName": "PRIYANKA MAHOR",
      "userId": "00901469365587543675"
     },
     "user_tz": -330
    },
    "id": "rE3Z725xRjfJ",
    "outputId": "920e913c-88bb-449a-9a90-5094f234f202"
   },
   "outputs": [],
   "source": [
    "#6 The Neural Network Model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_X[0]),), activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(train_Y[0]), activation = \"softmax\"))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(x=train_X, y=train_Y, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1654163038242,
     "user": {
      "displayName": "PRIYANKA MAHOR",
      "userId": "00901469365587543675"
     },
     "user_tz": -330
    },
    "id": "-A7GaQCTRnEn"
   },
   "outputs": [],
   "source": [
    "#7 Preprocessing the Input\n",
    "\n",
    "def clean_text(text): \n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "  return tokens\n",
    "\n",
    "def bag_of_words(text, vocab): \n",
    "  tokens = clean_text(text)\n",
    "  bow = [0] * len(vocab)\n",
    "  for w in tokens: \n",
    "    for idx, word in enumerate(vocab):\n",
    "      if word == w: \n",
    "        bow[idx] = 1\n",
    "  return np.array(bow)\n",
    "\n",
    "def pred_class(text, vocab, labels): \n",
    "  bow = bag_of_words(text, vocab)\n",
    "  result = model.predict(np.array([bow]))[0] #Extracting probabilities\n",
    "  thresh = 0.5\n",
    "  y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
    "  y_pred.sort(key=lambda x: x[1], reverse=True) #Sorting by values of probability in decreasing order\n",
    "  return_list = []\n",
    "  for r in y_pred:\n",
    "    return_list.append(labels[r[0]]) #Contains labels(tags) for highest probability \n",
    "  return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json): \n",
    "  if len(intents_list) == 0:\n",
    "    result = \"Sorry! I don't understand.\"\n",
    "  else:\n",
    "    tag = intents_list[0]\n",
    "    list_of_intents = intents_json[\"intents\"]\n",
    "    for i in list_of_intents: \n",
    "      if i[\"tag\"] == tag:\n",
    "        result = random.choice(i[\"responses\"])\n",
    "        break\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "gR6CsSsoRwfB"
   },
   "outputs": [],
   "source": [
    "# Interacting with the chatbot\n",
    "print(\"Press 1 if you don't want to chat with our ChatBot.\")\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    if message == \"1\":\n",
    "      break\n",
    "    intents = pred_class(message, words, classes)\n",
    "    result = get_response(intents, raw_data)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyvPcFPfSsrI"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOvXjEsP3e9zydOoMGV4Aan",
   "collapsed_sections": [],
   "name": "IPLBot.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
